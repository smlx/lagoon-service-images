# vi: ft=fluentd

# NOTE: when running in k8s a configmap is mounted over this file. That
# configmap is configured in the lagoon-logging helm chart.

<system>
  workers 2
</system>

<source>
  # fluentd parameters
  @type  forward
  @id    container
  tag    "lagoon.#{ENV['CLUSTER_NAME']}.raw"
</source>

# relabel router logs
# check app name first. if app name didn't match, set tag to container log.
<match lagoon.*.raw>
  @type rewrite_tag_filter
  <rule>
    key     $.kubernetes.labels.app
    pattern ^nginx-ingress$
    tag     "app-nginx-ingress"
  </rule>
  <rule>
    invert  true
    key     $.kubernetes.labels.app
    pattern ^nginx-ingress$
    tag     "lagoon.#{ENV['CLUSTER_NAME']}.container"
  </rule>
</match>
# check namespace_name. if it is okay too, tag as router log.
# if namespace didn't match, set tag to container log.
<match app-nginx-ingress>
  @type rewrite_tag_filter
  <rule>
    key     $.kubernetes.namespace_name
    pattern ^syn-nginx-ingress$
    tag     "lagoon.#{ENV['CLUSTER_NAME']}.router"
  </rule>
  <rule>
    invert  true
    key     $.kubernetes.namespace_name
    pattern ^syn-nginx-ingress$
    tag     "lagoon.#{ENV['CLUSTER_NAME']}.container"
  </rule>
</match>

# strip the duplicated log field from router logs
<filter lagoon.*.router>
  @type record_modifier
  remove_keys log
</filter>

# logs are now tagged appropriately, so route to labels based on the tag
<match lagoon.**>
  @type route
  # route _all_ logs container logs (even nginx-ingress) to @container
  <route lagoon.**>
    copy
    @label @container
  </route>
  # route just the router logs to @router
  <route lagoon.*.router>
    copy
    @label @router
  </route>
</match>

<label @container>
  # restructure so the kubernetes_metadata plugin can find the keys it needs
  <filter **>
    @type record_modifier
    remove_keys _dummy_
    <record>
      _dummy_ ${record['docker'] = {'container_id' => "#{record.dig('kubernetes','docker_id')}"}; nil}
    </record>
  </filter>
  # enrich with k8s metadata (will get the namespace labels)
  <filter **>
    @type kubernetes_metadata
    @log_level warn
    skip_container_metadata true
    skip_master_url         true
  </filter>
  # strip the duplicate information so that it doesn't appear in logs
  <filter **>
    @type record_modifier
    remove_keys docker
  </filter>
  # add the index_name
  <filter **>
    @type record_modifier
    <record>
      index_name container-logs-${record.dig('kubernetes','namespace_labels','lagoon.sh/project') || "#{record.dig('kubernetes','namespace_name') || 'unknown_project'}_#{ENV.fetch('CLUSTER_NAME','unknown_cluster')}"}-${Time.at(time).strftime("%Y.%m")}
    </record>
  </filter>
  # route to elasticsearch block
  <match **>
    @type relabel
    @label @elasticsearch
  </match>
</label>

<label @router>
  # strip the nginx-ingress namespace info and add enough dummy information
  # so that kubernetes_metadata plugin can get the namespace labels
  <filter **>
    @type record_modifier
    remove_keys _dummy_
    <record>
      _dummy_ ${record['kubernetes'] = {'namespace_name' => record['namespace'], 'pod_name' => 'nopod', 'container_name' => 'nocontainer'}; record['docker'] = {'container_id' => "#{record['namespace']}_#{record['ingress_name']}"}; nil}
    </record>
  </filter>
  # enrich with k8s metadata (will get the namespace labels)
  <filter **>
    @type kubernetes_metadata
    @log_level warn
    skip_container_metadata true
    skip_master_url         true
  </filter>
  # strip the dummy information so that it doesn't appear in logs
  <filter **>
    @type record_modifier
    remove_keys _dummy_,docker
    <record>
      _dummy_ ${record['kubernetes'].delete('pod_name'); record['kubernetes'].delete('container_name'); record['kubernetes'].delete('pod_id'); nil}
    </record>
  </filter>
  # add the index_name
  <filter **>
    @type record_modifier
    <record>
      index_name router-logs-${record.dig('kubernetes','namespace_labels','lagoon.sh/project') || "#{record.dig('kubernetes','namespace_name') || 'unknown_project'}_#{ENV.fetch('CLUSTER_NAME','unknown_cluster')}"}-${Time.at(time).strftime("%Y.%m")}
    </record>
  </filter>
  # route to elasticsearch block
  <match **>
    @type relabel
    @label @elasticsearch
  </match>
</label>

<label @elasticsearch>
  # send to elasticsearch
  <match **>
    @type elasticsearch
    @id out_elasticsearch
    # ingestion
    target_index_key index_name
    include_timestamp true
    time_key time
    # endpoint
    host "#{ENV['ELASTICSEARCH_HOST']}"
    port "#{ENV['ELASTICSEARCH_HOST_PORT']}"
    scheme "#{ENV.fetch('ELASTICSEARCH_SCHEME','http')}"
    ssl_min_version TLSv1_2
    ssl_max_version TLSv1_3
    user admin
    password "#{ENV['LOGSDB_ADMIN_PASSWORD']}"
    # endpoint error handling
    reconnect_on_error true
    reload_on_failure true
    request_timeout 600s
    slow_flush_log_threshold 300s
    # buffer chunks by tag
    <buffer tag>
      @type file
      path /fluentd/buffer/elasticsearch
      # buffer params (per worker)
      total_limit_size 8GB
      # flush params
      flush_thread_count 2
      overflow_action drop_oldest_chunk
    </buffer>
    # silence warnings (these have no effect)
    type_name _doc
    ssl_version TLSv1_2
  </match>
</label>
